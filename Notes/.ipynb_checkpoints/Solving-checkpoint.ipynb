{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the Top](#Top)\n",
    "\n",
    "# Data Analysis <a id='numerics'></a>\n",
    "\n",
    "The model is described by eq. 2, defining implicitly $Q$ as a function of $t$ with the free parameters $r_u$ and $Q_h$. It's convenient to scale the data so that we work with $q = Q/Q[0]$, then the equation to solve is\n",
    "\n",
    "$$\n",
    "        \\ln q + \\frac{q - 1}{q_h} = \\tau \\qquad (A1)\n",
    "$$\n",
    "\n",
    "where $\\tau = r_ut$. The solution is easily obtained with the Newton method: Introduce\n",
    "\n",
    "$$\n",
    "    f = \\ln q + \\frac{q - 1}{q_h} - \\tau\n",
    "$$\n",
    "\n",
    "and start with a guess for $q$. Then the next guess is\n",
    "\n",
    "$$\n",
    "    q \\to q - \\frac{f}{f'}\n",
    "$$\n",
    "and so on. Convergence is very rapid if the initial guess is reasonable. Since the solution for $\\tau = 0$ is $q = 1$, solving for an entire time series using the solution for each year as the initial quess for the next is quick and easy. \n",
    "\n",
    "The data were fitted with the python tool\n",
    "<a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\">curve_fit</a>, \n",
    "\n",
    "${\\tt curve\\_fit(f, xdata, ydata, p0=None, sigma=None, absolute\\_sigma=False, check\\_finite=True, bounds=(-inf, inf), method=None, jac=None, **kwargs)[source]}$\n",
    "\n",
    ">**sigma**: None or M-length sequence or MxM array, optional    \n",
    "\n",
    ">Determines the uncertainty in ydata. If we define residuals as r = ydata - f(xdata, *popt), then the interpretation of sigma depends on its number of dimensions:\n",
    "\n",
    ">A 1-d sigma should contain values of standard deviations of errors in ydata. In this case, the optimized function is chisq = sum((r / sigma) ** 2).\n",
    "\n",
    ">A 2-d sigma should contain the covariance matrix of errors in ydata. In this case, the optimized function is chisq = r.T @ inv(sigma) @ r.\n",
    "\n",
    ">**None (default) is equivalent of 1-d sigma filled with ones.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "which uses non-linear least squares to fit a function to data. The tool offers the potential to employ different solution methods, but I used it as is without delving into its inner working. The tool failed when attempting to solve for the data $Q$ itself but worked very well when using the scaled data $q = Q/Q[0]$. I invoked it without any initial guess for the parameters and only constrained them to be positive. The tool returns a solution for the optimal parameters $\\tt popt =[r_u, q_h]$ and the 2-d array $\\tt pcov$, the estimated covariance of popt. Its diagonals provide the variance of the parameter estimates, and the one standard deviation errors on the parameters are computed from $\\tt perr = np.sqrt(np.diag(pcov))$. From these I computed the relative errors $\\tt perr/popt$ reported in the model results.\n",
    "\n",
    "As noted, the only constraint I placed on the parameter search was $r_u, q_h > 0$. That resulted in one failure for dataset 'Long' (Japan) and 16 failures for the 'All67' dataset. Playing with the fits and placing the constraint $q_h \\ge 1$ I discovered that curve_fit obtained what it conidered acceptable fits for everything. All the successful fits from the $q_h \\ge 0$ fitting were unchanged. All the failures that became successes with $q_h \\ge 1$ had $q_h = 1$. The relative errors on the model parameters seem larger. For example, the 'Long' data for Japan has 57 points, the relative errors on the fitted $r_u$ and $q_h$ were 3.47E-01 and 4.63E-01, respectively. By comparison, the errors for the US, with 227 points, were only 1.93E-03 and 3.10E-02. Still, the average deviation of the Japan model from data was just 10.79%, the largest deviation 25.17%, implying a very respectable fit. \n",
    "\n",
    "Many issues to address:\n",
    "\n",
    "1. I have used curve_fit as a black-box and need a better understanding of its results. What is the confidence level we can place on the fitting results? Is there a measure of the meanigfulness of the fits? \n",
    "\n",
    "2. Can we understand, and get some control over, the dependence of modeling results on the data starting year (see the big difference for the UK and US between the 'Long' and 'All67' datasets)? \n",
    "\n",
    "3. Why does the fitting tool fail for some data with the $q_h \\ge 0$ constraint yet succeeds with $q_h \\ge 1$, deciding that a model with $q_h = 1$ is now acceptable? Why didn't curve_fit accept this solution when allowed to search all the way to $q_h \\ge 0$?  It would be good to understand this. While we have more than enough successful fits without the $q_h \\ge 1$ constraint, the failures include some of the world's biggest economies --- France, Germany, Italy and Japan. \n",
    "\n",
    "\n",
    "# 3-parameter fits <a id='3-parameters'></a>\n",
    "\n",
    "The solution in eq. 2 (and A1) always passes through the first data point $Q[0]$ (i.e., $q[0] = 1$ always). In principle, though, we should retain the freedom of an up-or-down shift. That is, instead of $Q[0]$ we should introduce an additional free parameter $Q_0$ and re-write eq. 2 as\n",
    "\n",
    "$$\n",
    "    \\ln\\frac{Q}{Q_0} + \\frac{Q - Q_0}{Q_h} = r_ut,             \\qquad (2')\n",
    "$$\n",
    "\n",
    "resulting in the three free parameters $r_u, Q_h$ and $Q_0$. Instead of eq. A1, we should then fit the scaled data $q = Q/Q[0]$ with\n",
    "\n",
    "$$\n",
    "        \\ln\\frac{q}{q_0} + \\frac{q - q_0}{q_h} = \\tau  \\qquad(A1')\n",
    "$$\n",
    "\n",
    "with $q_0 = Q_0/Q[0]$ as our 3rd parameter. It should be noted that the addition of the 3rd parameter can produce a change in shape, not just a shift. In a linear fit, relaxing the constraint $Q_0 = Q[0]$ allows a mere up-or-down shift on a linear scale. For an exponential, or pure power law, we again allow a shift on logarithmic scale. For a hindered solution, though, the initial, exponential part $Q_0\\exp(r_ut)$ is shifted on logarithmic scale while the hindered, linear part is $Q_0 + Q_h r_u t$, which is shifted linearly.   \n",
    "\n",
    "Fitting the US 'Long' data without a guess for the 3 paraeters failed, so I applied the following procedure: solve with 2 parameters, which works without an initial guess for $r_u$ and $q_h$ and uses a fixed $q_0 = 1$, then use the results as the initial guess for a 3-parameter fit. As seen below, this results in a successful fit but instead of improving the model it leads to a greatly inferior solution. This is puzzling.\n",
    "\n",
    "1. We need a more in-depth study of the 3-parameter fitting procedure. \n",
    "\n",
    "2. Why does the solution move away from a successful 2-parameter fit toward an inferior solution when we add a free parameter? Why don't we get a better fit with this additional freedom? \n",
    "\n",
    "\n",
    "[Back to the Top](#sec:Todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
